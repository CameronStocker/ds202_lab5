---
title: "Lab 5"
author: "Cameron Stocker and Tyler Kruzan"
date: "11/12/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,include=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
```

###1
```{r}
diabetes = read.table("diabetes.txt", header = TRUE)
str(diabetes)
```

###2
```{r}
diabetes[which(diabetes$frame == ''), 'frame'] = NA
diabetes$frame = droplevels(diabetes$frame)
```

###3
```{r}
diabetes_reduced = select(diabetes, -id, -bp.2s, -bp.2d)
```

###4
```{r}
diabetes_clean = diabetes_reduced[complete.cases(diabetes_reduced),]
```

###5
```{r}
summary(diabetes_clean)
```
You can see that Step 4 was completed correctly beause summary does not show any NA values. 

###6 & 7
```{r}
diabetes_clean = mutate(diabetes_clean, glyhb_star = log(glyhb))
ggplot(diabetes_clean, aes(glyhb_star)) + geom_histogram(bins = 10) + labs(x = "Glycosolated Hemoglobin", y= "Frequency", title = "Histogram of log(Glycosolated Hemoglobin)")
```

We have used a log transformation on the dataset. It is a relatively strong transformation which makes its interpretability harder. However, the transformation works because the graph is less right skewed. If you account for less bins, the distribution appears to much more normal.

###8 
```{r}
diabetes_clean %>% group_by(frame) %>% summarise(mean.glyhb = mean(glyhb_star))

diabetes_clean %>% group_by(waist) %>% summarise(mean.glyhb = mean(glyhb_star))

diabetes_clean %>% group_by(height) %>% summarise(mean.glyhb = mean(glyhb_star))

diabetes_clean %>% group_by(location) %>% summarise(mean.glyhb = mean(glyhb_star))

diabetes_clean %>% group_by(gender) %>% summarise(mean.glyhb = mean(glyhb_star))
```

`frame`, `waist`, & `height` all have a correlation, as the variable gets larger, the average Glycosolated Hemoglobin has a positive relationship and increases as well. When we summarized `location` & `gender` we found not relationship with the Glycosolated Hemoglobin. 
###10
```{r}
diabetes_clean %>% group_by(frame, location) %>% summarise(mean.glyhb_star=mean(glyhb_star)) %>%
  ggplot(aes(x = frame, y = mean.glyhb_star, color=location))+
  geom_point()
```

###11
```{r}
ggplot(diabetes_clean, aes(ratio, glyhb_star)) + geom_point() + labs(x = "Cholesterol/HDL Ratio", y = "log(Glycosolated Hemoglobin)", title = "Cholesterol/HDL Ratio vs. log(Glycosolated Hemoglobin)")
ggplot(diabetes_clean, aes(bp.1s, glyhb_star)) + geom_point() + labs(x = "First Systolic Blood Pressure", y = "log(Glycosolated Hemoglobin)", title = "First Systolic BP vs. log(Glycosolated Hemoglobin)")
ggplot(diabetes_clean, aes(age, glyhb_star)) + geom_point() + labs(x = "Age", y = "log(Glycosolated Hemoglobin)", title = "Age vs. log(Glycosolated Hemoglobin)")
ggplot(diabetes_clean, aes(hip, glyhb_star)) + geom_point() + labs(x = "Hip Measurement (inches)", y = "log(Glycosolated Hemoglobin)", title = "Hip Measurement vs. log(Glycosolated Hemoglobin)")
ggplot(diabetes_clean, aes(weight, glyhb_star)) + geom_point() + labs(x = "Weight (lbs)", y = "log(Glycosolated Hemoglobin)", title = "Weight vs. log(Glycosolated Hemoglobin)")
ggplot(diabetes_clean, aes(x = gender, y = glyhb_star)) + geom_boxplot() + labs(x = "Gender", y = "log(Glycosolated Hemoglobin)", title = "Gender vs. log(Glycosolated Hemoglobin) Boxplot")
```

###12
```{r}
ggplot(diabetes_clean,aes(y=weight,x=waist,alpha=0.5)) + geom_point() +  facet_wrap(~frame)+ geom_smooth() +labs(title = "Weight & Waist Length by Frame Size", x = "Waist Length (inches)", y = "Weight (lbs)", alpha= "Transparency") 

ggplot(diabetes_clean,aes(y=weight,x=waist,alpha=0.5)) + geom_point(aes(color = diabetes_clean$gender)) + facet_wrap(~frame) +labs(title = "Weight & Waist Length by Frame Size", x = "Waist Length (inches)", y = "Weight (lbs)",color='Gender', alpha= "Transparency") 
```

When visualizing the graphs above, we did not find any issues with overplotting. We added color and a smoothing line to create better insights for the relationship between the different frame sizes and the relationship between `frame` and `gender`

###13
The `gather` function takes multiple columns that should be values of a variables and converts them into 2 columns: key and value. Where the key is the name of the column for the new variable that has the values that you are wanting to transform and the value is the value of that variable that you are wanting to `gather`.

The `spread` function is the inverse of that as it is used when there is a column that contains values that should be variable headers instead. 

###14
`gather` & `spread` are not exact complements of each other, in fact they are opposites of each other. `spread` turns a key-value pair into a set of tidy-columns. `gather` turns a set of column names into a single key column.

###15
```{r}
fit = lm(glyhb_star ~ stab.glu + age + waist + ratio + factor(frame), data = diabetes_clean)
summary(fit)
```

We can see that this model has a significant relationship with the log of `glyhb`. We know this because we have a large F-statistic of 77.49 with a small p-value of <2.2e-16, which is less than any reasonable alpha. The adjusted r-squared tells us that 55.7% of the variation in the log of `glyhb` can be explained by this model. 

###17
  Below 
B0 = 0.8331: estimate of the mean value of log of glycosolated hemoglobin when stabilized glucose, age, waist, and cholesterol/HDL ratio are equal to 0 and the frame is large.

B1 = 0.0035: estimate of the change in the mean of log of glycosolated hemoglobin when stabilized glucose increases by 1 unit, holding all other covariates constant.

B2 = 0.0034: estimate of the change in the mean of log of glycosolated hemoglobin when age increases by 1 year, holding all other covariates constant.

B3 = 0.0048: estimate of the change in the mean of log of glycosolated hemoglobin when waist increases by 1 inches, holding all other covariates constant.

B4 = 0.0219: estimate of the change in the mean of log of glycosolated hemoglobin when Cholesterol/HDL ratio increases by 1 unit, holding all other covariates constant.

B5 = 0.0309: estimate of the difference in the predicted glycosolated hemoglobin when frame is medium instead of large, holding all other covariates constant.

B6 = 0.0132: estimate of the difference in the predicted glycosolated hemoglobin when frame is small instead of large, holding all other covariates constant.

###18
```{r, include=FALSE}
fit$fitted.values
```
```{r}
newdiabetes = data.frame(stab.glu=90, age=35,waist=30,ratio=5.1,frame="small")
predict.lm(fit, newdiabetes, interval = "prediction")
```

The estimated fitted values are estimates of the mean log of glycosolated hemoglobin for a given combination of stabilized glucose, age, waist, Cholesterol/HDL ratio, and frame. The Y^Y^ for this combination of values is 1.5364.

###19
**Pros of Linear Regression over KNN Regression**

Linear Regression will create a "shape" for the data because it is parametric. 
Parametric models are easier to interpret. 
Relatively quick compared to KNN regression.

**Cons of Linear Regression over KNN Regression**

The data cannot actually speak for itself, because we don't really know if the assumptions we made are true. 
Linear regression has the common problem of overfitting. 


###20
**Most surprising parts of Data Science: **

One of the main things that was truly surprising about Data Science has been the amount of prep work that goes into cleaning and tidying data. Before you can make analysis and give insights about the data you are working with, you must first make it workable. Learning about the various different packages supported by R for visualization has opened our mind about new ways that data can be visualized. 

**Most challenging parts of Data Science: **

Something that has been a little bit hard has been collaboration. You can collaborate with people via the internet, like getting help from StackOverflow, Reddit, or Piazza. However, in terms of real time collaboration with a team or partner is still tough because you must be in direct communication with the group before you push or pull your work to Github. Additionally, thinking outside of the box has proved to be a very valuable trait when working with data that seems elementary or dry. 

**Most enjoyable parts of Data Science:**

We believe that there is a moment where you can feel that you have transformed data into information. This moment is the reason why data scientists are in such high demand and we can give insights that somebody without a data science would not be able to achieve. Being able to explain things and teach people about various subjects using data is very empowering in our modern society where more than 2.5 quintillion bytes of data are collected everyday. 

**Owner Github Username: CameronStocker**
**Partner Github Username: tkruzan**
**Repo Name: ds202_lab5**
